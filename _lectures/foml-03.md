---
type: lecture
date: 2024-09-28
title: (foml-03) Neural Networks

# optional
# please use /static_files/notes directory to store notes
# thumbnail: /static_files/path/to/image.jpg

# optional
tldr: "Overview of Neural Networks, Regularization, Gradient Descent as a method of minimizing the loss function, and backpropagation."
  
# optional
# set it to true if you dont want this lecture to appear in the updates section
hide_from_announcments: true


# optional
links: 
    - url: https://docs.google.com/presentation/d/1cRebvCNyQJlocFSw7ZdAgM7NPZMNd49_6jfU4V1Vgj4/edit?usp=sharing
      name: NN
    - url: /static_files/presentations/dl-04.pdf
      name: Gradient Descent
    - url: /static_files/presentations/dl-05.pdf
      name: Backprop-1
    - url: /static_files/presentations/dl-06.pdf
      name: Backprop-2
    #- url: /static_files/presentations/code
    #  name: codes

---

**Suggested Reading**
- [Chapter 5 of PR and ML book by Christopher M Bishop](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf)
- [Chapter 2 in Michael Nielson's NNDL textbook](http://neuralnetworksanddeeplearning.com/chap2.html)
- [ML chapter from Goodfellow et al. DL book](https://www.deeplearningbook.org/contents/ml.html)
